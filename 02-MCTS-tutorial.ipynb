{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opened-pencil",
   "metadata": {},
   "source": [
    "# 02-Monte Carlos Tree Search (MCTS) Tutorial\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "\n",
    "-  The basics theory of Monte Carlos Tree Search (MCTS)\n",
    "-  A naive AI for tic-tac-toe\n",
    "-  MCTS for tic-tac-toe\n",
    "-  MCTS with UCB1 for tic-tac-toe\n",
    "-  Intuition behind Upper Confidence Bounded 1 (UCB1) (Why it works!)\n",
    "-  How to program a MCTS\n",
    "\n",
    "We will build the knowledge upon the real application (Tic Tac Toe) to get intuitions about what, how, and why MCTS works well in chess plays and other AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-advantage",
   "metadata": {},
   "source": [
    "## 1. Tic-Tac-Toe \n",
    "\n",
    "## 1.1 Introduction to Tic-Tac-Toe\n",
    "\n",
    "\n",
    "Tic-Tac-Toe is a game that two players take turns playing on a three-by-three board. One player plays Xs and the other Os until one player wins by placing three marks in a row, horizontally, vertically, or diagonally.\n",
    "\n",
    "Try this online game to get a feeling of Tic-Tac-Toe. [[link](https://playtictactoe.org/)]\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/online-game.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "## 1.2 Tic-Tac-Toe in Python\n",
    "\n",
    "In our github repository, there is a python script that implements tic-tac-toe in python. User can play with the designed AI player through command interface. The command interface is shown as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/tic-tac-toe-console.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "- Numbers on the left are the `row` number.\n",
    "- Numbers on the left are the `column` number.\n",
    "- User chooses the move position by input `#row, #column`, meaning the corresponding row and column numbers of the desired move position. For example, \"1, 2\" means choosing the move position at Row#1 and Column#2\n",
    "\n",
    "Let us run the following code to run tic-tac-toe in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "running-winter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 with X\n",
      "Player 2 with O\n",
      "\n",
      "       0       1       2\n",
      "\n",
      "   2   _       _       _    \n",
      "\n",
      "\n",
      "   1   _       _       _    \n",
      "\n",
      "\n",
      "   0   _       _       _    \n",
      "\n",
      "\n",
      "Your move: 1,2\n",
      "Player 1 with X\n",
      "Player 2 with O\n",
      "\n",
      "       0       1       2\n",
      "\n",
      "   2   _       _       _    \n",
      "\n",
      "\n",
      "   1   _       _       X    \n",
      "\n",
      "\n",
      "   0   _       _       _    \n",
      "\n",
      "\n",
      "Player 1 with X\n",
      "Player 2 with O\n",
      "\n",
      "       0       1       2\n",
      "\n",
      "   2   _       _       _    \n",
      "\n",
      "\n",
      "   1   _       O       X    \n",
      "\n",
      "\n",
      "   0   _       _       _    \n",
      "\n",
      "\n",
      "Your move: 1,1\n",
      "invalid move\n",
      "Your move: 2,1\n",
      "Player 1 with X\n",
      "Player 2 with O\n",
      "\n",
      "       0       1       2\n",
      "\n",
      "   2   _       X       _    \n",
      "\n",
      "\n",
      "   1   _       O       X    \n",
      "\n",
      "\n",
      "   0   _       _       _    \n",
      "\n",
      "\n",
      "Player 1 with X\n",
      "Player 2 with O\n",
      "\n",
      "       0       1       2\n",
      "\n",
      "   2   _       X       O    \n",
      "\n",
      "\n",
      "   1   _       O       X    \n",
      "\n",
      "\n",
      "   0   _       _       _    \n",
      "\n",
      "\n",
      "Your move: 1,0\n",
      "Player 1 with X\n",
      "Player 2 with O\n",
      "\n",
      "       0       1       2\n",
      "\n",
      "   2   _       X       O    \n",
      "\n",
      "\n",
      "   1   X       O       X    \n",
      "\n",
      "\n",
      "   0   _       _       _    \n",
      "\n",
      "\n",
      "Player 1 with X\n",
      "Player 2 with O\n",
      "\n",
      "       0       1       2\n",
      "\n",
      "   2   _       X       O    \n",
      "\n",
      "\n",
      "   1   X       O       X    \n",
      "\n",
      "\n",
      "   0   O       _       _    \n",
      "\n",
      "\n",
      "Game end. Winner is MCTS 2\n"
     ]
    }
   ],
   "source": [
    "# import local script\n",
    "from human_play import human_play\n",
    "\n",
    "n, width, height = 3,3,3 # line number that wins, board width, boad height\n",
    "ai_type = \"pure_mcts\"\n",
    "is_humanMoveFirst = True # set False if you want AI to play first\n",
    "human_play(n, width, height, ai_type, is_humanMoveFirst=is_humanMoveFirst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-livestock",
   "metadata": {},
   "source": [
    "In the above tic-tac-toe game, you played against a AI player with the Monte Carlo tree Search (MCTS). MCTS is a sophisticated AI that is widely used in game theory.\n",
    "\n",
    "In this tutorial, we will try to develop a Player with MCTS that will achieve human-level AI for the Tic-Tac-Toe Game.\n",
    "\n",
    "## 2. A Naive AI for solving Tic-Tac-Toe\n",
    "\n",
    "Here we try to explain what is state. The state is the representation information to describe the current situation of the environment. In the case of tic-tac-toe, the state is the current situation of the board. The following picture is three states in the chess game.\n",
    "\n",
    "<!--![dsfdf](media/tic-tac-toe-states.png)-->\n",
    "<p align=\"center\">\n",
    "<img src=\"media/tic-tac-toe-states.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "Note that the white \"X\" or \"O\" indicates the current turn for two players.\n",
    "\n",
    "Given a state, we have one or multiple choices of available actions corresponding to the state. Let us take the first state in the above picture as example. The available actions can be listed as following:\n",
    "<!--\n",
    "![dsfdf](media/tic-tac-toe-case1.png)-->\n",
    "<p align=\"center\">\n",
    "<img src=\"media/tic-tac-toe-case1.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "The player can put the \"O\"s to position #1 to #8 as the above picture shows. \n",
    "\n",
    "Ok, now if you are the player, which action among the available actions given this state will be the best or the smartest move?\n",
    "\n",
    "Perhaps the most natural thing that comes to your mind is to calculate the winning ratio of action given a particular state.\n",
    "But how to calculate the winning ratio? Maybe one of the feasible ways is to do simulations to estimate and foresee the winning ratio of actions. \n",
    "Just like Doctor Strange!\n",
    "<!--\n",
    "![](media/doctor-strange.gif)\n",
    "[<img src=\"media/doctor-strange.gif\" width=\"250\"/>](media/doctor-strange.gif)-->\n",
    "<p align=\"center\">\n",
    "<img src=\"media/doctor-strange.gif\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "For example, given action 1, we can do hundreds of simulations to estimate the winning ratio. Eventually, you might calculate the winning ratios for states like this\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/tic-tac-toe-winning-ratio2.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "The number in black in the above picture is the probability of winning ratio. Note that probabilities of all actions sum to 1. Based on the result, you can make a \"smart\" moving, the one with the highest probability. That is Action #5, with a probability of 0.89, in the above picture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-destination",
   "metadata": {},
   "source": [
    "## 3. Monte Carlos Tree Search for solving Tic-Tac-Toe\n",
    "\n",
    "So far, you might feel that the naive approach might be too unrealistic. You are right. One of the most significant drawbacks is that the simulations grow with the number of all state-action pairs in the game. We can calculate the number of all state-action pairs for tic-tac-toe as\n",
    "\n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=9*8*7...*1=9!=362880\">\n",
    "\n",
    "If you do 100 simulations for an action-state pair, you need to do 362880*100 simulations. That might be durable for some of the computers. But given a 10 by 10 board, the number of state-pair is factorial of 100, 9.332622e+157, which is definitely out of computational power. In fact, given an \"n by n\" board, the number of state-pair is <img src=\"https://render.githubusercontent.com/render/math?math=n^2!\">\n",
    "\n",
    "So are there some methods to create a more data-efficient AI? You know when professional players play a chess game, they will plan their moves a few steps ahead. In such a method, they can try to foresee the resultant future states which action might lead to. In AI theory, there is a similar method that is able to foresee the states. That is Monte Carlos Tree Search.\n",
    "\n",
    "### 3.1 Basics of Tree\n",
    "\n",
    "Let us try to foresee by planning the move.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/tree.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "In the above picture, we illustrate the planning using a tree structure. Since the tree is large, only a part of the entire tree is shown in the picture. To simplify the representation of the above picture, we use the following illustration, simply using nodes and edges.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/tree-representation.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "The node represents state and the edge represents available action. This allows us to analyze the resultant states of each action and its further consequence. Let us look deeper into the tree.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/tree-deep.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "As you might notice, each state has a blue number. The blue number means the winning probability of \"O\" given the current state (or the value in reinforcement learning). The above picture shows the deep tree with 3 step-look ahead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-chick",
   "metadata": {},
   "source": [
    "### 3.2 Process of MCTS \n",
    "#### 3.2.1 How to traverse the tree <span style=\"color:blue\">(Selection)</span>\n",
    "\n",
    "So the next question might be how to jump from the parent node to the child node. Since the blue number for each state means the winning ratio of Player \"O\", the rules of traverse is easily set as:\n",
    "\n",
    "- Choose the available action with Maximum winning ratios of \"O\" at the turn of \"O\".\n",
    "- Choose the available action with Minimum winning ratios of \"O\" at the turn of \"X\".\n",
    "\n",
    "Such a rule is known as \"MinMax\", which is a famous algorithm in game theory.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/minmax.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "Why do we try to minimize the winning ratios of \"O\" at the turn of \"X\"? Because we try to make the player \"X\", the opponent of \"O\", be \"smart\" on his decision. An analogy I like to think about is that people will tend to play against the opponent is better or at least the same with their own for chess play to improve their skill of playing chess. In general, the \"MinMax\" algorithm or the rules of traverse can impose some constraints for the exploration that finds the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-endorsement",
   "metadata": {},
   "source": [
    "#### 3.2.2 How to expand or \"grow\" the tree <span style=\"color:blue\">(Expansion)</span>\n",
    "\n",
    " After you traverse the tree using the \"MinMax\" algorithm, you will reach leaf nodes (the bottom of the tree). If you want to grow the tree to see a few more move ahead, you need to expand (or in other words \"grow\") the tree. It is easy to expand the tree as:\n",
    " \n",
    " - Add new nodes (representing new states) to the leaf nodes.\n",
    " - Initialize the nodes with initial values. In our case, just we initialize the value of the winning ratio to zero.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/tree-grow.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "[wiki link](https://zh.wikipedia.org/wiki/%E6%9E%81%E5%B0%8F%E5%8C%96%E6%9E%81%E5%A4%A7%E7%AE%97%E6%B3%95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-analyst",
   "metadata": {},
   "source": [
    "#### 3.2.3 Simulation<span style=\"color:blue\"> (Rollout)</span>  \n",
    "\n",
    "From the perspective of frequentists, the probability of winning ratio can be estimated using statistics, such as the Monte Carlo methods. Let us check the following picture\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/probability-calculation.png\" alt=\"drawing\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "n is the number of game plays starting at the current state, t is the number of winning plays starting at the current state. The estimated probability of winning ratio is formulated as\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=\\hat{p} = t/ n\">\n",
    "</p>\n",
    "\n",
    "As the `n` grows larger, the estimated probability get more accurate. So we might need to do simulations as many as possible to get an accurate winning ratio. Next, we will explain how to increase the number of simulated plays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-visibility",
   "metadata": {},
   "source": [
    " For MCTS, after the traverse and expansion process, we will reach the leaf node. We will do rollout (or simulation in other words) when we find n=0.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/rollout.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "In the rollout, the learned policy will play against a random policy, which might be the policy that gives uniform random moves. After the simulation, we record the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-caution",
   "metadata": {},
   "source": [
    "#### 3.2.4 Update Result<span style=\"color:blue\">(Backpropagation)</span> \n",
    "\n",
    "After a rollout, we will update the tree based on the result of the rollout. \n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"media/backpropagation.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "As the above picture shows, we divide the cases into two:\n",
    "\n",
    "- Case 1 (Player win): n and t are incremented by 1 for the rollout state, its parent state, and its ancestral states.\n",
    "- Case 2 (Player lose): n is incremented by 1 for the rollout state, its parent state, and its ancestral states.\n",
    "\n",
    "You might notice that the value is updated backward for the tree, which is similar to the backpropagation of gradient descent for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-friendship",
   "metadata": {},
   "source": [
    "#### 3.2.5 Summary: A full process of MCTS \n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"media/mcts-process.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "The process of MCTS for one playout is set as the following sequences: \n",
    "\n",
    "- Selection -> Expasion -> Rollout -> Backpropagation. \n",
    "\n",
    "For MCTS, in order to approximate the probabilities of winning ratios accurately, we will try to execute playouts as many as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-browser",
   "metadata": {},
   "source": [
    "## 4 Monte Carlos Tree Search with UCB1\n",
    "\n",
    "### 4.1 Introduction to MCTS with UCB1\n",
    "After you play with MCTS for a while, you will find out there is a problem with the original MCTS algorithm. Specifically, the \"MinMax\" algorithm will always choose actions with the maximum probability. At the beginning of playouts when the tree is small, the estimated probability might be not accurate due to its small sampling number n. This might lead to choosing sub-optimal actions, while other actions that might be optimal will be not selected. In the terminology of reinforcement learning, the sub-optimal actions are \"exploited\" too much, leaving other actions \"un-explored\".\n",
    "\n",
    "\"Exploitation and Exploration\" is a well-known trade-off in reinforcement learning. One might need to balance the tradeoff in order to find an optimal policy. \n",
    "\n",
    "UCB1 (Upper Confidence Bound) is able to balance the \"Exploitation\" and \"Exploration\" for MCTS. The UCB1 value for a state can be formulated as\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=UCB = {\\frac{t}{n}} %2B 2\\sqrt{\\frac{ln(N)}{n}}\">\n",
    "</p>\n",
    "\n",
    "where N is the n value for its parent states.\n",
    "\n",
    "\"Minmax\" algorithm with UCB1 can be formulated as\n",
    "\n",
    "- Choose the available action with Maximum UCB for your turn.\n",
    "- Choose the available action with Minimum UCB for the opponent's turn.\n",
    "\n",
    "For the sake of computational convenience, we maximize the negative probability for the opponent's turns. The \"Minmax\" algorithm with UCB1 can be reformulated as\n",
    "\n",
    "- <img src=\"https://render.githubusercontent.com/render/math?math=UCB = {\\frac{t}{n}} * (-1)^{k%2B1} %2B 2\\sqrt{\\frac{ln(N)}{n}}\">\n",
    "- Choose the available action with Maximum UCB.\n",
    "\n",
    "Where K is the number of turns, here we assume that the first turn is your turn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-attack",
   "metadata": {},
   "source": [
    "### 4.2 Intuition of UCB1 \n",
    "\n",
    "Why does UCB1 work for balancing the trade-off of \"exploration\" and \"exploitation\"?\n",
    "\n",
    "Let look at the following cases:\n",
    "\n",
    "-Case 1:\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"media/ucb-case1.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "At the bottom, there are two leaf nodes that have been visited since n is not equal to zero, While the other leaf nodes remain un-visited. You will find that UCB values for un-visited leaf nodes tend to infinity. Following the rule of \"MinMax\", those nodes that have never been visited before will be selected. In this case, the UCB values will encourage \"exploration\".\n",
    "\n",
    "\n",
    "-Case 2:\n",
    "<p align=\"center\">\n",
    " <img src=\"media/ucb-case2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "When all the sibling node has been visited many time, the difference among their the terms in UCB <img src=\"https://render.githubusercontent.com/render/math?math=2\\sqrt{\\frac{ln(N)}{n}}\"> will be small. However, UCB will weight more on the difference of the other term <img src=\"https://render.githubusercontent.com/render/math?math={\\frac{t}{n}} * (-1)^{k%2B1}\">. In this case, the UCB values will encourage \"exploitation\".\n",
    "\n",
    "In conclusion, UCB will encourage \"Exploration\" when the number of visits, n, for sibling nodes is small while encouraging \"Exploitation\" when n is large. This makes sense on some level since, at the beginning of playouts, the estimation of the winning ratio is not too accurate. Encouraging \"Exploitation\" will prevent the cases that miss out on the nodes leading to the optimal policy. When n is large, the estimation becomes more accurate. So encouraging \"Exploitation\" will make the tree grow deeper, which contributes to the data efficiency of finding the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-feeding",
   "metadata": {},
   "source": [
    "## 5 Program a MCTS\n",
    "\n",
    "In the repository, we have a file called [mcts_pure.py](https://github.com/linhongbin-ws/MAEG-3080-project/blob/master/mcts_pure.py), which implemented the MCTS AI player. Let us walk through the code.\n",
    "\n",
    "### 5.1 Class `TreeNode()`\n",
    "\n",
    "Since the node of tree has some similar properties, we can use a class to program it. Here we explain the attributes method and variables for the tree node class. \n",
    "\n",
    "---\n",
    "\n",
    "We will start with the **init** function:\n",
    "\n",
    "```py\n",
    "class TreeNode(object):\n",
    "    \"\"\"A node in the MCTS tree. Each node keeps track of its own value Q,\n",
    "    prior probability P, and its visit-count-adjusted prior score u.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent, prior_p):\n",
    "        self._parent = parent\n",
    "        self._children = {}  # a map from action to TreeNode\n",
    "        self._n_visits = 0\n",
    "        self._Q = 0\n",
    "        self._u = 0\n",
    "        self._P = prior_p\n",
    "```\n",
    "- `self._parent`: The address of parent treeNode, `None` if it is a root.\n",
    "- `self._children = {}`: A List of addresses of children treeNode.\n",
    "- `self._n_visits`: The number of visits for current node (`n` in this tutorial)\n",
    "- `self._Q`: The winning ratio:\n",
    " \n",
    "<img style=\"margin:0px 0 0 100px\" src=\"https://render.githubusercontent.com/render/math?math={\\frac{t}{n}} * (-1)^{k%2B1} \">\n",
    "\n",
    "- `self._u`: the second term in the UCB equation:\n",
    "\n",
    "<img style=\"margin:0px 0 0 100px\" src=\"https://render.githubusercontent.com/render/math?math=2\\sqrt{\\frac{ln(N)}{n}}  \">\n",
    "\n",
    "- `self._P`: the prior winning ratio of this node.\n",
    "\n",
    "\n",
    "```py\n",
    "    def get_value(self, c_puct):\n",
    "        \"\"\"Calculate and return the value for this node.\n",
    "        It is a combination of leaf evaluations Q, and this node's prior\n",
    "        adjusted for its visit count, u.\n",
    "        c_puct: a number in (0, inf) controlling the relative impact of\n",
    "            value Q, and prior probability P, on this node's score.\n",
    "        \"\"\"\n",
    "        self._u = (c_puct * self._P *\n",
    "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
    "        return self._Q + self._u\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if leaf node (i.e. no nodes below this have been expanded).\n",
    "        \"\"\"\n",
    "        return self._children == {}\n",
    "\n",
    "    def is_root(self):\n",
    "        return self._parent is None\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "There are also some **elementary functions** for Treenode:\n",
    "\n",
    "```py\n",
    "    def get_value(self, c_puct):\n",
    "        \"\"\"Calculate and return the value for this node.\n",
    "        It is a combination of leaf evaluations Q, and this node's prior\n",
    "        adjusted for its visit count, u.\n",
    "        c_puct: a number in (0, inf) controlling the relative impact of\n",
    "            value Q, and prior probability P, on this node's score.\n",
    "        \"\"\"\n",
    "        self._u = (c_puct * self._P *\n",
    "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
    "        return self._Q + self._u\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if leaf node (i.e. no nodes below this have been expanded).\n",
    "        \"\"\"\n",
    "        return self._children == {}\n",
    "\n",
    "    def is_root(self):\n",
    "        return self._parent is None\n",
    "```\n",
    "\n",
    "- `get_value`: Get UCB value, c_puct is a weight parameter, set to 2 for the equation \n",
    "<img style=\"margin:0px 0 0 100px\" src=\"https://render.githubusercontent.com/render/math?math=2\\sqrt{\\frac{ln(N)}{n}}  \">\n",
    "- `is_leaf`: Check if the node is the leaf node\n",
    "- `is_root`: Check if the node is root node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-little",
   "metadata": {},
   "source": [
    "### 5.2 Selection\n",
    "```py\n",
    "def select(self, c_puct):\n",
    "    \"\"\"Select action among children that gives maximum action value Q\n",
    "    plus bonus u(P).\n",
    "    Return: A tuple of (action, next_node)\n",
    "    \"\"\"\n",
    "    return max(self._children.items(),\n",
    "               key=lambda act_node: act_node[1].get_value(c_puct))\n",
    "``` \n",
    "\n",
    "`select` function will select the children node with maximum UCB value among all childeren nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-necessity",
   "metadata": {},
   "source": [
    "### 5.3 Expansion\n",
    "```py\n",
    "def expand(self, action_priors):\n",
    "    \"\"\"Expand tree by creating new children.\n",
    "    action_priors: a list of tuples of actions and their prior probability\n",
    "        according to the policy function.\n",
    "    \"\"\"\n",
    "    for action, prob in action_priors:\n",
    "        if action not in self._children:\n",
    "            self._children[action] = TreeNode(self, prob)\n",
    "```\n",
    "\n",
    "`expand` function will create new children nodes corresponding to available actions given the state of current node. Arguement `action_priors` is a list of tupes: (`action`, `prob`), where `action` is the Action No. and `prob` is the prior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-possession",
   "metadata": {},
   "source": [
    "### 5.4 Backpropagation\n",
    "\n",
    "```py\n",
    "def update(self, leaf_value):\n",
    "    \"\"\"Update node values from leaf evaluation.\n",
    "    leaf_value: the value of subtree evaluation from the current player's\n",
    "        perspective.\n",
    "    \"\"\"\n",
    "    # Count visit.\n",
    "    self._n_visits += 1\n",
    "    # Update Q, a running average of values for all visits.\n",
    "    self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n",
    "\n",
    "def update_recursive(self, leaf_value):\n",
    "    \"\"\"Like a call to update(), but applied recursively for all ancestors.\n",
    "    \"\"\"\n",
    "    # If it is not root, this node's parent should be updated first.\n",
    "    if self._parent:\n",
    "        self._parent.update_recursive(-leaf_value)\n",
    "    self.update(leaf_value)\n",
    "```\n",
    "\n",
    "A recursive function is implemented in the above code. This [link](https://techterms.com/definition/recursive_function) and [wiki](https://en.wikipedia.org/wiki/Recursion_(computer_science)) explain what the recursive function is.\n",
    "\n",
    "- `update_recursive()` will update the current node and its parent node in a recursive manner, `leaf_value` is the value for updating\n",
    "- `update()` will update the node value.\n",
    "```py\n",
    "self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n",
    "```\n",
    "will update self._Q by averaging all its historical values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-province",
   "metadata": {},
   "source": [
    "### 5.2 Class `MCTS()`\n",
    "\n",
    "After we build the class for treeNode, we can create a class to implement MCTS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-newspaper",
   "metadata": {},
   "source": [
    "#### 5.2.1 `Init()` \n",
    "\n",
    "```py\n",
    "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n",
    "        \"\"\"\n",
    "        policy_value_fn: a function that takes in a board state and outputs\n",
    "            a list of (action, probability) tuples and also a score in [-1, 1]\n",
    "            (i.e. the expected value of the end game score from the current\n",
    "            player's perspective) for the current player.\n",
    "        c_puct: a number in (0, inf) that controls how quickly exploration\n",
    "            converges to the maximum-value policy. A higher value means\n",
    "            relying on the prior more.\n",
    "        \"\"\"\n",
    "        self._root = TreeNode(None, 1.0)\n",
    "        self._policy = policy_value_fn\n",
    "        self._c_puct = c_puct\n",
    "        self._n_playout = n_playout\n",
    "\n",
    "```\n",
    "\n",
    "- `c_puct`: a user param to control exploration\n",
    "- `n_playout`: number of playouts\n",
    "- `policy_value_fn`: A prior policy function. In this code we set a prior policy with uniform distribution\n",
    "\n",
    "In the code, a prior policy with uniform distribution is programmed as:\n",
    "\n",
    "```py\n",
    "def policy_value_fn(board):\n",
    "    \"\"\"a function that takes in a state and outputs a list of (action, probability)\n",
    "    tuples and a score for the state\"\"\"\n",
    "    # return uniform probabilities and 0 score for pure MCTS\n",
    "    action_probs = np.ones(len(board.availables))/len(board.availables)\n",
    "    return zip(board.availables, action_probs), 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-elizabeth",
   "metadata": {},
   "source": [
    "### 5.2.2 Rollout\n",
    "\n",
    "Inside the process of MCTS for each playout, rollout is executed. Here are the code of rollout:\n",
    "```py\n",
    "    def _evaluate_rollout(self, state, limit=1000):\n",
    "        \"\"\"Use the rollout policy to play until the end of the game,\n",
    "        returning +1 if the current player wins, -1 if the opponent wins,\n",
    "        and 0 if it is a tie.\n",
    "        \"\"\"\n",
    "        player = state.get_current_player()\n",
    "        for i in range(limit):\n",
    "            end, winner = state.game_end()\n",
    "            if end:\n",
    "                break\n",
    "            action_probs = rollout_policy_fn(state)\n",
    "            max_action = max(action_probs, key=itemgetter(1))[0]\n",
    "            state.do_move(max_action)\n",
    "        else:\n",
    "            # If no break from the loop, issue a warning.\n",
    "            print(\"WARNING: rollout reached move limit\")\n",
    "        if winner == -1:  # tie\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 if winner == player else -1\n",
    "```\n",
    "\n",
    "In the rollout, the program will simluate 1000 rounds of tic-tac-toe given a specific state. Rollout policy will be used for self-playing. In the pure MCTS, the code will use the rollout policy with random move:\n",
    "\n",
    "```py\n",
    "def rollout_policy_fn(board):\n",
    "    \"\"\"a coarse, fast version of policy_fn used in the rollout phase.\"\"\"\n",
    "    # rollout randomly\n",
    "    action_probs = np.random.rand(len(board.availables))\n",
    "    return zip(board.availables, action_probs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-tunnel",
   "metadata": {},
   "source": [
    "### 5.2.3 Playout\n",
    "```py\n",
    "    def _playout(self, state):\n",
    "        \"\"\"Run a single playout from the root to the leaf, getting a value at\n",
    "        the leaf and propagating it back through its parents.\n",
    "        State is modified in-place, so a copy must be provided.\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        while(1):\n",
    "            if node.is_leaf():\n",
    "\n",
    "                break\n",
    "            # Greedily select next move.\n",
    "            action, node = node.select(self._c_puct)\n",
    "            state.do_move(action)\n",
    "\n",
    "        action_probs, _ = self._policy(state)\n",
    "        # Check for end of game\n",
    "        end, winner = state.game_end()\n",
    "        if not end:\n",
    "            node.expand(action_probs)\n",
    "        # Evaluate the leaf node by random rollout\n",
    "        leaf_value = self._evaluate_rollout(state)\n",
    "        # Update value and visit count of nodes in this traversal.\n",
    "        node.update_recursive(-leaf_value)\n",
    "```\n",
    "\n",
    "This part of code will execute the process of MCTS for each playout. The process of MCTS for one playout is set as the following sequences: \n",
    "\n",
    "- Selection -> Expasion -> Rollout -> Backpropagation. \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"media/mcts-process.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-relief",
   "metadata": {},
   "source": [
    "### 5.2.4 `get_move()`\n",
    "\n",
    "```py\n",
    "def get_move(self, state):\n",
    "    \"\"\"Runs all playouts sequentially and returns the most visited action.\n",
    "    state: the current game state\n",
    "    Return: the selected action\n",
    "    \"\"\"\n",
    "    for n in range(self._n_playout):\n",
    "        state_copy = copy.deepcopy(state)\n",
    "        self._playout(state_copy)\n",
    "    return max(self._root._children.items(),\n",
    "               key=lambda act_node: act_node[1]._n_visits)[0]\n",
    "```\n",
    "This function is a high-level function that compute the move with the highest probability of winning ratio. By default, we will run 2000 playouts to get the most accurate estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-unknown",
   "metadata": {},
   "source": [
    "### 5.2.5 `update_with_move()`\n",
    "```py\n",
    "def update_with_move(self, last_move):\n",
    "    \"\"\"Step forward in the tree, keeping everything we already know\n",
    "    about the subtree.\n",
    "    \"\"\"\n",
    "    if last_move in self._root._children:\n",
    "        self._root = self._root._children[last_move]\n",
    "        self._root._parent = None\n",
    "    else:\n",
    "        self._root = TreeNode(None, 1.0)\n",
    "```\n",
    "\n",
    "This function will step into the next move while keeping the structure of the sub-tree starting from the next move. For instance:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/step-into-tree.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-chapel",
   "metadata": {},
   "source": [
    "## 5.3 AI Player using MCTS algorithm\n",
    "\n",
    "```py\n",
    "class MCTSPlayer(object):\n",
    "    \"\"\"AI player based on MCTS\"\"\"\n",
    "    def __init__(self, c_puct=5, n_playout=2000):\n",
    "        self.mcts = MCTS(policy_value_fn, c_puct, n_playout)\n",
    "\n",
    "    def set_player_ind(self, p):\n",
    "        self.player = p\n",
    "\n",
    "    def reset_player(self):\n",
    "        self.mcts.update_with_move(-1)\n",
    "\n",
    "    def get_action(self, board):\n",
    "        sensible_moves = board.availables\n",
    "        if len(sensible_moves) > 0:\n",
    "            move = self.mcts.get_move(board)\n",
    "            self.mcts.update_with_move(-1)\n",
    "            return move\n",
    "        else:\n",
    "            print(\"WARNING: the board is full\")\n",
    "```\n",
    "\n",
    "- `__init__()`: create a MCTS class\n",
    "- `set_player_ind()`: set the ID of Player\n",
    "- `reset_player()`: reset MCST by discarding all tree stucture.\n",
    "- `get_action`: get the action with hightest winning ratio using MCTS. Tree will be reset by calling `self.mcts.update_with_move(-1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-migration",
   "metadata": {},
   "source": [
    "## 6 Homework (Practice)\n",
    "\n",
    "1. play tic-tac-toe in your python console\n",
    "\n",
    "```py\n",
    "# import local script\n",
    "from human_play import human_play\n",
    "\n",
    "n, width, height = 3,3,3 # line number that wins, board width, boad height\n",
    "ai_type = \"pure_mcts\"\n",
    "is_humanMoveFirst = True # set False if you want AI to play first\n",
    "human_play(n, width, height, ai_type, is_humanMoveFirst=is_humanMoveFirst)\n",
    "```\n",
    "\n",
    "2. reivew the codes: \n",
    "\n",
    "- [mcts_pure.py](https://github.com/linhongbin-ws/MAEG-3080-project/blob/master/mcts_pure.py)\n",
    "- [human_play.py](https://github.com/linhongbin-ws/MAEG-3080-project/blob/master/human_play.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-female",
   "metadata": {},
   "source": [
    "**Reference**:\n",
    "\n",
    "1. Clear Explanation by John Levine: [link](https://www.youtube.com/watch?v=UXW2yZndl7U&t=2s&ab_channel=JohnLevine)\n",
    "\n",
    "1. Chinese Course on MCTS: [link](https://www.youtube.com/watch?v=niIaKaWIRX0&ab_channel=%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6MOOC-%E6%85%95%E8%AF%BE)\n",
    "\n",
    "1. Survey Paper for MCTS (Deep Understanding on MCTS and its variations): [link](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6145622&casa_token=AecTrST5MJYAAAAA:1UepYH0lA9-jdodOaItjidj0ie8kcKFAH65qh4F3AzkX1wiWrfNj4lb5Um-w7RJChEu0heo3&tag=1)\n",
    "\n",
    "1. Simple code of MCTS [link](https://github.com/haroldsultan/MCTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-scratch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
